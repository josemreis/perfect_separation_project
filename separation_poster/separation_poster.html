<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">


<!--
Font-awesome icons ie github or twitter
-->
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/brands.css" integrity="sha384-n9+6/aSqa9lBidZMRCQHTHKJscPq6NW4pCQBiMmHdUCvPN8ZOg2zJJTkC7WIezWv" crossorigin="anonymous">

<!--
Google fonts api stuff
-->



<title>Separation and non-identification in logistic regression models: a simulation study</title>

<script src="separation_poster_files/header-attrs-2.6/header-attrs.js"></script>
<script src="separation_poster_files/kePrint-0.0.1/kePrint.js"></script>
<link href="separation_poster_files/lightable-0.0.1/lightable.css" rel="stylesheet" />




<style>
@page {
size: 45in 38in;
margin: 0;
padding: 0;
}
body {
margin: 0;
font-size: 22pt;
width: 45in;
height: 38in;
padding: 0;
text-align: justify;
font-family: Palatino;
}
.poster_wrap {
width: 45in;
height: 38in;
padding: 0cm;
}
.title_container {
width: 45in;
height: calc(38in * 0.15);
overflow: hidden;
background-color: #0b4545;
border: 0 solid #0b4545;
}
.logo_left {
float: left;
width: 10%;
height: 100%;
background-color: #0b4545;
display: flex;
align-items: center;
justify-content: center;
}
.logo_right {
float: right;
width: 10%;
height: 100%;
background-color: #0b4545;
display: flex;
align-items: center;
justify-content: center;
}
.poster_title {
text-align: center;
position: relative;
float: left;
width: 80%;
height: 100%;
color: #FFFFFF;
top: 50%;
transform: translateY(-50%);
-webkit-transform: translateY(-50%);
}
#title {
font-family: Palatino;
}
/* unvisited link */
a:link {
color: #cc0000;
}
.mybreak {
  break-before: column;
}
/* visited link */
a:visited {
color: #cc0000;
}

/* mouse over link */
a:hover {
color: #cc0000;
}

/* selected link */
a:active {
color: #cc0000;
}
.poster_body {
-webkit-column-count: 4; /* Chrome, Safari, Opera */
-moz-column-count: 4; /* Firefox */
column-count: 4;
-webkit-column-fill: auto;
-moz-column-fill: auto;
column-fill: auto;
-webkit-column-rule-width: 1mm;
-moz-column-rule-width: 1mm;
column-rule-width: 1mm;
-webkit-column-rule-style: dashed;
-moz-column-rule-style: dashed;
column-rule-style: dashed;
-webkit-column-rule-color: #0b4545;
-moz-column-rule-color: #0b4545;
column-rule-color: #0b4545;
column-gap: 1em;
padding-left: 0.5em;
padding-right: 0.5em;
height: 100%;
color: #000000
background-color: #ffffff;
}
.poster_title h1 {
font-size: 75pt;
margin: 0;
border: 0;
font-weight: normal;
}
.poster_body_wrap{
width: calc(45in + 0 + 0);
height: calc(38in * 0.83);
padding-top: calc(38in * 0.005);
padding-bottom: calc(38in * 0.01);
background-color: #ffffff;
}
.poster_title h3 {
color: #ffffff;
font-size: 50pt;
margin: 0;
border: 0;
font-weight: normal;
}
.poster_title h3 > sup {
  font-size: 35pt;
  margin-left: 0.02em;
}
.poster_title h5 {
color: #FFFFFF;
font-size: 35pt;
margin: 0;
border: 0;
font-weight: normal;
}
img {
margin-top: 2cm;
margin-bottom: 0;
}
.section {
  padding: 0.2em;
}
.poster_body h1 {
text-align: center;
color: #FFFFFF;
font-size: 65pt;
border: 2mm solid #0b4545;
background-color: #0b4545;
border-radius: 4mm 0mm;
margin-top: 2mm;
margin-bottom: 2mm;
font-weight: normal;
}
.poster_body h2 {
color: #000000;
font-size: 40pt;
padding-left: 4mm;
font-weight: normal;
}
.span {
width: 200%;
}
/* center align leaflet map,
from https://stackoverflow.com/questions/52112119/center-leaflet-in-a-rmarkdown-document */
.html-widget {
margin: auto;
position: sticky;
margin-top: 2cm;
margin-bottom: 2cm;
}
.leaflet.html-widget.html-widget-static-bound.leaflet-container.leaflet-touch.leaflet-fade-anim.leaflet-grab.leaflet-touch-drag.leaflet-touch-zoom {
position: sticky;
width: 100%;
}
pre.sourceCode.r {
background-color: #dddddd40;
border-radius: 4mm;
padding: 4mm;
width: 75%;
/* align-items: center; */
margin: auto;
padding-left: 2cm;
}
code.sourceCode.r{
background-color: transparent;
font-size: 20pt;
border-radius: 2mm;
}
.caption {
font-size: 25pt;
}
.table caption {
font-size: 25pt;
padding-bottom: 3mm;

}
code {
font-size: 1em;
font-family: monospace;
background-color: #00808024;
color: #0b4545;
padding: 1.2mm;
border-radius: 2mm;
}
.poster_title code {
font-size: 1em;
}
table {
font-size: 40px;
margin: auto;
border-top: 3px solid #666;
border-bottom: 3px solid #666;
}
table thead th {
border-bottom: 3px solid #ddd;
}
td {
padding: 8px;
}
th {
padding: 15px;
}
caption {
margin-bottom: 10px;
}
.poster_body p {
margin-right: 4mm;
margin-left: 4mm;
margin-top: 6mm;
margin-bottom: 10mm;
}
.poster_body ol {
margin-right: 4mm;
margin-left: 4mm;
}
#ul {
margin-right: 4mm;
margin-left: 4mm;
}
.references p {
font-size: 20pt;
}
.orcid img {
  width: 1em;
}
</style>
</head>
<body>


<div class="poster_wrap">
<div class="title_container">
<!-- Left Logo  -->
<div class="logo_left">
</div>
<!-- Poster Title -->
<div class= "poster_title">
<h1 id="title">Separation and non-identification in logistic regression models: a simulation study</h1>
<h3 id="author">José Reis<sup></sup></h3><br>
<h5 id="affiliation"><sup></sup> data and code at <a href="https://github.com/josemreis/separation_project" class="uri">https://github.com/josemreis/separation_project</a></h5>
</div>
<!-- Right Logo  -->
<div class="logo_right">
</div>
</div>

<div class='poster_body_wrap'>
<div class='poster_body'>
<div id="logistic-regression" class="section level1">
<h1>Logistic regression</h1>
<p>Logistic regression is a generalized linear model for binary data. It models the conditional distribution of <span class="math inline">\(Y\)</span>, a random vector composed of dichotomous observations <span class="math inline">\((y_1, ...,y_n) \in \{0,1\}\)</span>, given <span class="math inline">\(X\)</span>, a vector of co-variate variables <span class="math inline">\((x_i, ...,x_n)\)</span>, as a Bernoulli distribution, or <span class="math inline">\(Binomial(1, \pi)\)</span>.</p>
<p><span class="math display">\[
y_i|x_i \sim \text{Binomial}(1, \pi_i) 
\]</span>
Where <span class="math inline">\(\pi_i\)</span> is assumed to be a function of a linear component a linear component, <span class="math inline">\(\alpha + \beta X_i\)</span>, which is modeled using a logit link function.</p>
<p><span class="math display">\[
g(\pi_i) = logit \: \pi_i = log \: \frac{\pi_i}{1 - \pi_i} = \alpha + \beta x \\
\text{where} \\
Pr(Y=1|X) = logit^{-1} \pi_i = \frac{e^{\alpha + \beta x}}{1 + e^{\alpha + \beta x}}
\]</span></p>
<p>To estimate <span class="math inline">\(\beta_i\)</span> researchers find the coefficient vector which maximizes log-likelihood function.</p>
<p><span class="math display">\[
l(\beta|x_1, ..., x_n) = \sum_{i=1}^n y_i \: log \: \pi_i + (1-y_i) \: log \: (1-\pi_i)
\]</span></p>
<p>In most cases, the MLE must be approximated using some numerical method (e.g. Newton-Raphson algorithm).</p>
</div>
<div id="separation" class="section level1">
<h1>Separation</h1>
<p><strong>Complete separation</strong> occurs if there is a vector of parameter values <span class="math inline">\(\beta = (\beta_1, ..., \beta_n)\)</span> which partitions the explanatory value at <span class="math inline">\(T\)</span> in such a way that in one side the dependent variable is always <span class="math inline">\(y_i = 1\)</span> and on the other it is always <span class="math inline">\(y_i = 0\)</span> (see <span class="citation">Albert and Anderson (1984)</span>). That is, the linear component can completely discriminate <span class="math inline">\(y\)</span>.</p>
<p><span class="math display">\[
\beta x_i &gt; T \: \text{whenever} \: y_i = 1,\\
\beta x_i &lt; T \: \text{whenever} \: y_i = 0
\]</span></p>
<p><strong>Quasi complete separation</strong> occurs whenever there is at least one vector of parameter values <span class="math inline">\(\beta = (\beta_1, ..., \beta_n)\)</span> which partitions the explanatory value at <span class="math inline">\(T\)</span> in such a way that the following equality holds for at least one <span class="math inline">\(y_i\)</span> (<em>idem</em>).</p>
<p><span class="math display">\[
\beta x_i \geq  T \: \text{whenever} \: y_i = 1 \\ 
\beta x_i \leq  T \: \text{whenever} \: y_i = 0 
\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-31"></span>
<img src="separation_poster_files/figure-html/unnamed-chunk-31-1.png" alt="Perfect and quasi-perfect separation" width="960" />
<p class="caption">
Figure 1: Perfect and quasi-perfect separation
</p>
</div>
<div id="the-problem" class="section level2">
<h2>The problem</h2>
<ul>
<li>MLE goes to infinity</li>
<li>Nevertheless, most statistical software will <em>converge</em> before reaching the MLE and report biased log odds ratios and SEs</li>
<li>p-values and Wald confidence intervals cannot be relied on for hypothesis testing</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-32"></span>
<img src="separation_poster_files/figure-html/unnamed-chunk-32-1.png" alt="Negative log-likelihoods" width="960" />
<p class="caption">
Figure 2: Negative log-likelihoods
</p>
</div>
</div>
</div>
<div id="proposed-solutions" class="section level1">
<h1>Proposed solutions</h1>
<div id="penalized-maximum-likelihood" class="section level2">
<h2>Penalized maximum likelihood</h2>
<p>While MLE estimates do not exist under separation, Firth <span class="citation">(1993)</span> showed that by adding the log of the square root of the fisher information matrix as a penalty to the log likelihood function</p>
<p><span class="math display">\[
l_f = l(\beta) + \frac{1}{2} log |I(\beta)|
\]</span></p>
<p>it becomes is strictly concave and so the penalized log-likelihood will converge on one unique maximum penalized likelihood estimate.</p>
</div>
<div id="bayesian-approaches" class="section level2">
<h2>Bayesian approaches</h2>
<p>In a nutshell, Bayesian approaches focus in general focus on estimating the probability distribution of the coefficient given the observed data. This is estimated via the Bayes theorem</p>
<p><span class="math display">\[
\overbrace{Pr(\theta|D)}^\text{Posterior} = \frac{\overbrace{Pr(D|\theta)}^\text{Likelihood} \times\overbrace{Pr(\theta)}^\text{Prior}}{\underbrace{Pr(D)}_\text{Average Likelihood}}
\\
\]</span>
Bayesian methods combine the model of the data (likelihood) with <em>prior information</em> about <span class="math inline">\(\beta\)</span>. The prior distribution can be used to constrain our likelihood estimates. Researchers have studied how prior distributions of the coefficients may curb separation issues. They have focused on which is the best weakly informed prior for separation issues - i.e. default options.</p>
<ul>
<li>Some literature discusses using <span class="math inline">\(Cauchy(x_0 = 0, \gamma = 2.5)\)</span> distribution <span class="citation">(Gelman et al. 2008)</span> as default</li>
<li>Others argue for a <span class="math inline">\(t(df = 7)\)</span> distribution <span class="citation">(Ghosh, Li, and Mitra 2015)</span></li>
<li>Alternatively, a normal distribution centered around 0 and with an <a href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations">informed</a> choice of standard deviation <span class="citation">(Rainey 2016)</span></li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-33"></span>
<img src="separation_poster_files/figure-html/unnamed-chunk-33-1.png" alt="Priors discussed in the literature" width="960" />
<p class="caption">
Figure 3: Priors discussed in the literature
</p>
</div>
</div>
</div>
<div id="simulation" class="section level1">
<h1>Simulation</h1>
<p>The robustness of the following methods was put to test in a simulation set up.</p>
<table class=" lightable-classic-2" style="font-size: 20px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-34">Table 1: </span>Methods Evaluated
</caption>
<thead>
<tr>
<th style="text-align:left;">
Method
</th>
<th style="text-align:left;">
Point.Estimate
</th>
<th style="text-align:left;">
CI
</th>
<th style="text-align:left;">
Software
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Maximum Likelihood Logistic Regression
</td>
<td style="text-align:left;">
MLE
</td>
<td style="text-align:left;">
Not applicable
</td>
<td style="text-align:left;">
stats::glm() (R 4.0.3)
</td>
</tr>
<tr>
<td style="text-align:left;">
Penalized Maximum Likelihood Logistic Regression
</td>
<td style="text-align:left;">
PMLE
</td>
<td style="text-align:left;">
Profile Likelihood
</td>
<td style="text-align:left;">
logistif::logistf() (R 4.0.3)
</td>
</tr>
<tr>
<td style="text-align:left;">
Bayesian Logistic Regression with <span class="math inline">\(\beta \sim Cauchy(0, 2,5) \: \alpha \sim t(0, 0, 10)\)</span>
</td>
<td style="text-align:left;">
MAP
</td>
<td style="text-align:left;">
Highest Density Probability Interval
</td>
<td style="text-align:left;">
rstanarm::stan_glm() (R 4.0.3)
</td>
</tr>
<tr>
<td style="text-align:left;">
Bayesian Logistic Regression with <span class="math inline">\(\beta \sim t(10,0,0) \: \alpha \sim t(0, 0, 10)\)</span>
</td>
<td style="text-align:left;">
MAP
</td>
<td style="text-align:left;">
Highest Density Probability Interval
</td>
<td style="text-align:left;">
rstanarm::stan_glm() (R 4.0.3)
</td>
</tr>
<tr>
<td style="text-align:left;">
Bayesian Logistic Regression with <span class="math inline">\(\beta \sim Normal(0, 2.5) \: \alpha \sim t(0, 0, 10)\)</span>
</td>
<td style="text-align:left;">
MAP
</td>
<td style="text-align:left;">
Highest Density Probability Interval
</td>
<td style="text-align:left;">
rstanarm::stan_glm() (R 4.0.3)
</td>
</tr>
</tbody>
</table>
<p>Each procedure was evaluated based on:</p>
<ol style="list-style-type: decimal">
<li><strong>Bias</strong>, <span class="math inline">\(Bias = E[\hat{\beta}] - \beta\)</span></li>
<li><strong>Mean-squared error</strong>, <span class="math inline">\(MSE = E[Bias(\hat{\beta})^2]\)</span></li>
<li><strong>Coverage probability of the confidence intervals</strong>, <em>i.e.</em> proportion of CIs containing the true parameter</li>
</ol>
<div id="data-generating-process" class="section level2">
<h2>Data generating process</h2>
<ol style="list-style-type: decimal">
<li>Randomly sample 100 parameters between 2 and 6, <span class="math inline">\(\beta_{sim}\)</span></li>
<li>Simulate a population with 1.000.000 observations using the following data generating process</li>
</ol>
<p><span class="math display">\[
y|x \sim Binomial(1, \pi) \\
\pi = logit^{-1} \pi = \frac{exp(\alpha + \beta_{sim} X)}{1 + exp(\alpha + \beta_{sim} X)} \\
X \sim N(0,1) \\
\alpha = 0 
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>From each population draw <span class="math inline">\(\frac{1.000.000}{n_{sample}}\)</span> samples, where <span class="math inline">\(n_{sample} = \{25, 50, 75\}\)</span>. For each sample check whether perfect separation or quasi-perfect separation occurred <span class="citation">(Kosmidis and Schumacher 2020)</span></li>
<li>Keep only samples with separation issues</li>
<li>Keep only populations which contain at least 1000 samples with separation</li>
<li>Use a random sample of 1000</li>
</ol>
<p>We ended up with 55 simulated populations each with a different parameter and 1000 simulated samples (N = 55000) which we used to study the properties of the estimation methods.</p>
</div>
<div id="results" class="section level2">
<h2>Results</h2>
<table class=" lightable-classic-2" style="font-size: 20px; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-36">Table 2: </span>findings
</caption>
<thead>
<tr>
<th style="text-align:left;">
estimation method
</th>
<th style="text-align:right;">
Bias averaged over the 55 populations (#samples = 1000)
</th>
<th style="text-align:right;">
MSE averaged over the 55 populations (#samples = 1000)
</th>
<th style="text-align:right;">
Coverage probability averaged over the 55 populations (#samples = 1000)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Bayesian logit model, beta ~ t(7, 0)
</td>
<td style="text-align:right;">
0.3281
</td>
<td style="text-align:right;">
0.7348
</td>
<td style="text-align:right;">
1.0000
</td>
</tr>
<tr>
<td style="text-align:left;">
Bayesian logit model, beta ~ normal(0, 2.5)
</td>
<td style="text-align:right;">
-0.3639
</td>
<td style="text-align:right;">
0.6325
</td>
<td style="text-align:right;">
1.0000
</td>
</tr>
<tr>
<td style="text-align:left;">
Penalized Maximum Likelihood Estimation
</td>
<td style="text-align:right;">
2.0369
</td>
<td style="text-align:right;">
10.6321
</td>
<td style="text-align:right;">
0.9993
</td>
</tr>
<tr>
<td style="text-align:left;">
Bayesian logit model, beta ~ Cauchy(0, 2.5)
</td>
<td style="text-align:right;">
21.3363
</td>
<td style="text-align:right;">
508.9328
</td>
<td style="text-align:right;">
0.7794
</td>
</tr>
<tr>
<td style="text-align:left;">
Maximum Likelihood Estimation
</td>
<td style="text-align:right;">
237.1292
</td>
<td style="text-align:right;">
156665.5319
</td>
<td style="text-align:right;">
NA
</td>
</tr>
</tbody>
</table>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-37"></span>
<img src="separation_poster_files/figure-html/unnamed-chunk-37-1.png" alt="Expected value of beta and simulation standard error" width="960" />
<p class="caption">
Figure 4: Expected value of beta and simulation standard error
</p>
</div>
<ul>
<li>With <em>separated samples</em> MLE provides very biased estimates of the population parameter</li>
<li>PML and assigning prior distributions to the parameter curb this problem</li>
<li>Bayesian approaches yielded less biased estimates than PML, except in the case of <span class="math inline">\(\beta \sim Cauchy(0, 2.5)\)</span></li>
</ul>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-albert1984existence" class="csl-entry">
Albert, Adelin, and John A Anderson. 1984. <span>“On the Existence of Maximum Likelihood Estimates in Logistic Regression Models.”</span> <em>Biometrika</em> 71 (1): 1–10.
</div>
<div id="ref-firth1993bias" class="csl-entry">
Firth, David. 1993. <span>“Bias Reduction of Maximum Likelihood Estimates.”</span> <em>Biometrika</em> 80 (1): 27–38.
</div>
<div id="ref-gelman2008weakly" class="csl-entry">
Gelman, Andrew, Aleks Jakulin, Maria Grazia Pittau, Yu-Sung Su, and others. 2008. <span>“A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models.”</span> <em>Annals of Applied Statistics</em> 2 (4): 1360–83.
</div>
<div id="ref-ghosh2015use" class="csl-entry">
Ghosh, Joyee, Yingbo Li, and Robin Mitra. 2015. <span>“On the Use of Cauchy Prior Distributions for Bayesian Logistic Regression.”</span> <em>arXiv Preprint arXiv:1507.07170</em>.
</div>
<div id="ref-detect" class="csl-entry">
Kosmidis, Ioannis, and Dirk Schumacher. 2020. <em>Detectseparation: Detect and Check for Separation and Infinite Maximum Likelihood Estimates</em>. <a href="https://CRAN.R-project.org/package=detectseparation">https://CRAN.R-project.org/package=detectseparation</a>.
</div>
<div id="ref-rainey2016dealing" class="csl-entry">
Rainey, Carlisle. 2016. <span>“Dealing with Separation in Logistic Regression Models.”</span> <em>Political Analysis</em>, 339–55.
</div>
</div>
</div>
</div>
</div>

</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
var script = document.createElement("script");
script.type = "text/javascript";
var src = "true";
if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
if (location.protocol !== "file:" && /^https?:/.test(src))
src = src.replace(/^https?:/, '');
script.src = src;
document.getElementsByTagName("head")[0].appendChild(script);
})();
</script>


</body>
</html>
